{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load The Base Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b2f0e-c3af-4be6-84c4-47f8b0d45302",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style, cm\n",
    "import matplotlib.tri as mtri\n",
    "\n",
    "\n",
    "# Load the database\n",
    "data = pd.read_csv('Base13.csv')\n",
    "\n",
    "# Print the data clusters\n",
    "plt.figure(figsize=(10,10))\n",
    "x = []\n",
    "x.append(data.iloc[:,0:2].values)\n",
    "X = np.array(data.values[:, :2])\n",
    "x.append(data.iloc[:,2:3].values)\n",
    "\n",
    "for i in range(len(X)):\n",
    "    plt.plot(X[i][0], X[i][1], color=\"red\")\n",
    "plt.show()\n",
    "\n",
    "X= np.array(data.values[:, :2])\n",
    "label= np.array(data.values[:, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coesion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, silhouette_score, pairwise_distances\n",
    "\n",
    "def coesion_value(X, labels):\n",
    "    each_label= np.unique(labels)\n",
    "    valu= 0\n",
    "    for lab in each_label:\n",
    "        if lab!=-1:\n",
    "            indexes= np.where(labels == lab)\n",
    "            indexes= indexes[0]\n",
    "            subX= np.take(X, indexes, axis=0)\n",
    "            valu+= np.sum(pairwise_distances(subX, metric='sqeuclidean', n_jobs=-1))\n",
    "    return np.sqrt(valu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def separation_value(X, labels):\n",
    "    each_label= np.unique(labels)\n",
    "    value= 0\n",
    "\n",
    "    for lab in each_label:\n",
    "        if lab!=-1:\n",
    "            index_X= np.where(labels == lab)\n",
    "            index_X= index_X[0]\n",
    "\n",
    "            index_Y= np.where(np.logical_and(labels != lab, labels!=-1))\n",
    "            index_Y= index_Y[0]\n",
    "\n",
    "            subX= np.take(X, index_X, axis=0)\n",
    "            subY= np.take(X, index_Y, axis=0)\n",
    "\n",
    "            # distance between points\n",
    "            value+= np.sum(pairwise_distances(subX, subY, metric='sqeuclidean', n_jobs=-1))\n",
    "    return np.sqrt(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "def entropy_value(X, label_class, label_dataset):\n",
    "    cluster_labels= np.unique(label_dataset)\n",
    "    entropia_valueE= 0\n",
    "    for label in cluster_labels:\n",
    "\n",
    "        entropia_c= 0\n",
    "\n",
    "        cluster_index= np.where(label_dataset == label)\n",
    "        cluster_index= cluster_index[0]\n",
    "        cluster_classes= np.take(label_class, cluster_index)\n",
    "\n",
    "        classes, points_c= np.unique(cluster_classes, return_counts=True)\n",
    "        cluster_sum= np.sum(points_c)\n",
    "        if classes[0]==-1:\n",
    "            points_c= points_c[1:]\n",
    "        points_c= points_c / cluster_sum\n",
    "\n",
    "        for p in points_c:\n",
    "            entropia_c+= p * np.log2(p)\n",
    "            entropia_valueE-= entropia_c\n",
    "\n",
    "            entropia_valueE/= len(cluster_labels)\n",
    "    return entropia_valueE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the parameters\n",
    "kmeans_parameters = {\n",
    "    \"n_clusters\": [i for i in range(2, 9)],\n",
    "    \"max_iter\": [j for j in range(1, 11)]\n",
    "}\n",
    "\n",
    "# Define the size of the parameters\n",
    "cluster_size= np.shape(kmeans_parameters['n_clusters'])[0]\n",
    "iter_size= np.shape(kmeans_parameters['max_iter'])[0]\n",
    "\n",
    "# Generate the matrix of values\n",
    "kmeans_value_coesao= np.zeros((cluster_size, iter_size))\n",
    "kmeans_value_separabilidade= np.zeros((cluster_size, iter_size))\n",
    "kmeans_value_entropia= np.zeros((cluster_size, iter_size))\n",
    "kmeans_value_silhueta= np.zeros((cluster_size, iter_size))\n",
    "\n",
    "i= 0\n",
    "j= 0\n",
    "\n",
    "# Calculate the values\n",
    "for n_cluster in kmeans_parameters[\"n_clusters\"]:\n",
    "    j= 0\n",
    "    for iteration in kmeans_parameters[\"max_iter\"]:\n",
    "        kmc= KMeans(n_clusters=n_cluster, max_iter=iteration).fit(X)\n",
    "        kmc_labels= kmc.labels_\n",
    "        coesao = coesion_value(X, kmc_labels)\n",
    "        separabilidade = separation_value(X, kmc_labels)\n",
    "        entropia = entropy_value(X, kmc_labels, label)\n",
    "        silhueta = silhouette_score(X, kmc_labels, metric='euclidean')\n",
    "        homogeneidade = sk.metrics.homogeneity_score(label, kmc_labels)\n",
    "        completude = sk.metrics.completeness_score(label, kmc_labels)\n",
    "        rand_score = sk.metrics.rand_score(label, kmc_labels)\n",
    "\n",
    "        kmeans_value_coesao[i, j]= coesao\n",
    "        kmeans_value_separabilidade[i, j]= separabilidade\n",
    "        kmeans_value_entropia[i, j]= entropia\n",
    "        kmeans_value_silhueta[i, j]= silhueta\n",
    "        kmeans_value_homogeneidade[i, j]= homogeneidade\n",
    "        kmeans_value_completude[i, j]= completude\n",
    "        kmeans_value_rand[i, j]= rand_score\n",
    "\n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "# Get the shape of the matrix\n",
    "template_shape= np.shape(kmeans_value_coesao)\n",
    "\n",
    "# Get the best scores\n",
    "maxScore_coesao_kmeans= np.min(kmeans_value_coesao)\n",
    "minScore_separabilidade_kmeans= np.max(kmeans_value_separabilidade)\n",
    "minScore_entropia_kmeans= np.min(kmeans_value_entropia )\n",
    "maxScore_silhueta_kmeans = np.max(kmeans_value_silhueta)\n",
    "maxScore_homogeneidade_kmeans = np.max(kmeans_value_homogeneidade)\n",
    "maxScore_completude_kmeans = np.max(kmeans_value_completude)\n",
    "maxScore_rand_kmeans = np.max(kmeans_value_rand)\n",
    "\n",
    "# Get the parameters values\n",
    "index_coesion = np.unravel_index(np.argmin(kmeans_value_coesao), template_shape)\n",
    "index_separation = np.unravel_index(np.argmax(kmeans_value_separabilidade), template_shape)\n",
    "index_entropy = np.unravel_index(np.argmin(kmeans_value_entropia ), template_shape)\n",
    "index_silhouette = np.unravel_index(np.argmax(kmeans_value_silhueta), template_shape)\n",
    "index_homogeneidade = np.unravel_index(np.argmax(kmeans_value_homogeneidade), template_shape)\n",
    "index_completude = np.unravel_index(np.argmax(kmeans_value_completude), template_shape)\n",
    "index_rand = np.unravel_index(np.argmax(kmeans_value_rand), template_shape)\n",
    "\n",
    "# Get the best parameters\n",
    "bestIter_coesion = kmeans_parameters['max_iter'][index_coesion[1]]\n",
    "bestN_coesion = kmeans_parameters['n_clusters'][index_coesion[0]]\n",
    "\n",
    "bestIter_separation = kmeans_parameters['max_iter'][index_separation[1]]\n",
    "bestN_separation = kmeans_parameters['n_clusters'][index_separation[0]]\n",
    "\n",
    "bestIter_entropy = kmeans_parameters['max_iter'][index_entropy[1]]\n",
    "bestN_entropy = kmeans_parameters['n_clusters'][index_entropy[0]]\n",
    "\n",
    "bestIter_silhouette = kmeans_parameters['max_iter'][index_silhouette[1]]\n",
    "bestN_silhouette = kmeans_parameters['n_clusters'][index_silhouette[0]]\n",
    "\n",
    "bestIter_homogeneidade = kmeans_parameters['max_iter'][index_homogeneidade[1]]\n",
    "bestN_homogeneidade = kmeans_parameters['n_clusters'][index_homogeneidade[0]]\n",
    "\n",
    "bestIter_completude = kmeans_parameters['max_iter'][index_completude[1]]\n",
    "bestN_completude = kmeans_parameters['n_clusters'][index_completude[0]]\n",
    "\n",
    "bestIter_rand = kmeans_parameters['max_iter'][index_rand[1]]\n",
    "bestN_rand = kmeans_parameters['n_clusters'][index_rand[0]]\n",
    "\n",
    "# Show the best Results\n",
    "print(\"KMEANS\")\n",
    "print(f\"\\nCoesion\\n Best Score = {maxScore_coesao_kmeans}\\n {bestIter_coesion} Iterations {bestN_coesion} Centroids\")\n",
    "print(f\"\\nSeparation:\\n Best Score ->{minScore_separabilidade_kmeans}\\n {bestIter_separation} Iterations {bestN_separation} Centroids\")\n",
    "print(f\"\\nEntropy:\\n Best Score = {minScore_entropia_kmeans}\\n {bestIter_entropy} Iterations {bestN_entropy} Centroids\")\n",
    "print(f\"\\nSilhouette:\\n Best Score = {maxScore_silhueta_kmeans}\\n {bestIter_silhouette} Iterations {bestN_silhouette} Centroids\")\n",
    "print(f\"\\nHomogeneity:\\n Best Score = {maxScore_homogeneidade_kmeans}\\n {bestIter_homogeneidade} Iterations {bestN_homogeneidade} Centroids\")\n",
    "print(f\"\\nCompleteness:\\n Best Score = {maxScore_completude_kmeans}\\n {bestIter_completude} Iterations {bestN_completude} Centroids\")\n",
    "print(f\"\\nRand:\\n Best Score = {maxScore_rand_kmeans}\\n {bestIter_rand} Iterations {bestN_rand} Centroids\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DB Scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Define the parameters\n",
    "dbsacn_p= {\n",
    "    \"eps\": [i/100 for i in range(1, 16)],\n",
    "    \"min_samples\": [i for i in range(3, 14)]\n",
    "}\n",
    "\n",
    "# List of validated parameters\n",
    "validated_params = []\n",
    "\n",
    "# Find the best parameters\n",
    "for epsx in dbsacn_p[\"eps\"]:\n",
    "    j= 0\n",
    "    for min_sample in dbsacn_p[\"min_samples\"]:\n",
    "        dbs= DBSCAN(eps=epsx, min_samples=min_sample).fit(X)\n",
    "        dbs_labels= dbs.labels_\n",
    "\n",
    "        # Save the validated parameters\n",
    "        if(len(np.unique(dbs_labels)) <= 3):\n",
    "            validated_params.append([epsx, min_sample])\n",
    "\n",
    "# Get the unique values\n",
    "validated_params = np.array(validated_params)\n",
    "unique_eps= np.unique(validated_params[:, 0])\n",
    "unique_samples= np.unique(validated_params[:, 1])\n",
    "\n",
    "# Get the matrix size\n",
    "matrix_size= ((np.shape(unique_eps)[0], np.shape(unique_samples)[0]))\n",
    "\n",
    "# Generate the matrix of values\n",
    "dbsacn_value_coesao= np.zeros(matrix_size)\n",
    "dbscan_value_separabilidade= np.zeros(matrix_size)\n",
    "dbscan_value_entropia= np.zeros(matrix_size)\n",
    "dbscan_value_silhueta= np.zeros(matrix_size)\n",
    "\n",
    "i= 0\n",
    "j= 0\n",
    "\n",
    "# Calculate the values\n",
    "for eps in unique_eps:\n",
    "    j = 0\n",
    "    for sample in unique_samples:\n",
    "        if [eps, sample] in validated_params.tolist():\n",
    "          dbs = DBSCAN(eps=eps, min_samples=int(sample), n_jobs=-1).fit(X)\n",
    "          dbs_labels= dbs.labels_\n",
    "\n",
    "          # Ensure there's more than one cluster\n",
    "          if len(set(dbs_labels)) > 1:  \n",
    "              coesion = coesion_value(X, dbs_labels)\n",
    "              entropy = entropy_value(X, dbs_labels, label)\n",
    "              separation = entropy_value(X, dbs_labels, label)\n",
    "              silhouette = silhouette_score(X, dbs_labels, metric='euclidean')\n",
    "              homogeneidade = sk.metrics.homogeneity_score(label, dbs_labels)\n",
    "              completude = sk.metrics.completeness_score(label, dbs_labels)\n",
    "              rand_score = sk.metrics.rand_score(label, dbs_labels)\n",
    "\n",
    "              dbsacn_value_coesao[i, j] = coesion\n",
    "              dbscan_value_separabilidade[i, j] = separation\n",
    "              dbscan_value_entropia[i, j] = entropy\n",
    "              dbscan_value_silhueta[i, j] = silhouette\n",
    "              dbscan_value_homogeneidade[i, j] = homogeneidade\n",
    "              dbscan_value_completude[i, j] = completude\n",
    "              dbscan_value_rand[i, j] = rand_score\n",
    "              \n",
    "          else:\n",
    "              dbsacn_value_coesao[i, j] = float(\"inf\")\n",
    "              dbscan_value_separabilidade[i, j] = float(\"inf\")\n",
    "              dbscan_value_entropia[i, j] = float(\"inf\")\n",
    "              dbscan_value_silhueta[i, j] = float(\"inf\")\n",
    "              dbscan_value_homogeneidade[i, j] = float(\"inf\")\n",
    "              dbscan_value_completude[i, j] = float(\"inf\")\n",
    "              dbscan_value_rand[i, j] = float(\"inf\")\n",
    "\n",
    "        j+=1\n",
    "    i+=1\n",
    "\n",
    "# Get the best values\n",
    "dbsacn_value_coesao_= np.where(dbsacn_value_coesao == float(\"inf\"), np.min(dbsacn_value_coesao)*.99, dbsacn_value_coesao)\n",
    "dbscan_value_separabilidade_= np.where(dbscan_value_separabilidade == float(\"inf\"), np.min(dbscan_value_separabilidade)*.99, dbscan_value_separabilidade)\n",
    "dbscan_value_entropia_= np.where(dbscan_value_entropia == float(\"inf\"), np.min(dbscan_value_entropia)*.99, dbscan_value_entropia)\n",
    "dbscan_value_silhueta = np.where(dbscan_value_silhueta == float(\"inf\"), np.min(dbscan_value_silhueta)*.99, dbscan_value_silhueta)\n",
    "dbacan_value_homogeneidade= np.where(dbsacn_value_coesao == float(\"inf\"), 0, dbsacn_value_coesao)\n",
    "dbacan_value_coesao= np.where(dbsacn_value_coesao == float(\"inf\"), 0, dbsacn_value_coesao)\n",
    "dbacan_value_completude= np.where(dbsacn_value_coesao == float(\"inf\"), 0, dbsacn_value_coesao)\n",
    "dbacan_value_rand= np.where(dbsacn_value_coesao == float(\"inf\"), 0, dbsacn_value_coesao)\n",
    "\n",
    "dbscan_value_separabilidade= np.where(dbsacn_value_coesao == float(\"inf\"), 0, dbsacn_value_coesao)\n",
    "dbscan_value_silhueta = np.where(dbscan_value_silhueta == float(\"inf\"), 0, dbscan_value_silhueta)\n",
    "template_shape= np.shape(dbsacn_value_coesao)\n",
    "\n",
    "maxScore_coesao_dbs= np.min(dbsacn_value_coesao)\n",
    "minScore_separabilidade_dbs= np.max(dbscan_value_separabilidade)\n",
    "minScore_entropia_dbs= np.min(dbscan_value_entropia)\n",
    "maxScore_silhueta_dbs = np.max(dbscan_value_silhueta)\n",
    "maxScore_homogeneidade_dbs = np.max(dbacan_value_homogeneidade)\n",
    "maxScore_completude_dbs = np.max(dbacan_value_completude)\n",
    "maxScore_rand_dbs = np.max(dbacan_value_rand)\n",
    "\n",
    "index_coesao= np.unravel_index(np.argmin(dbsacn_value_coesao), template_shape)\n",
    "index_separabilidade= np.unravel_index(np.argmax(dbscan_value_separabilidade), template_shape)\n",
    "index_entropia= np.unravel_index(np.argmin(dbscan_value_entropia), template_shape)\n",
    "index_silhueta= np.unravel_index(np.argmax(dbscan_value_silhueta), template_shape)\n",
    "index_homogeneidade= np.unravel_index(np.argmax(dbacan_value_homogeneidade), template_shape)\n",
    "index_completude= np.unravel_index(np.argmax(dbacan_value_completude), template_shape)\n",
    "index_rand= np.unravel_index(np.argmax(dbacan_value_rand), template_shape)\n",
    "\n",
    "bestEps_coesao= unique_eps[index_coesao[0]]\n",
    "bestMinSamples_coesao= unique_samples[index_coesao[1]]\n",
    "\n",
    "bestEps_separabilidade= unique_eps[index_separabilidade[0]]\n",
    "bestMinSamples_separabilidade= unique_samples[index_separabilidade[1]]\n",
    "\n",
    "bestEps_entropia= unique_eps[index_entropia[0]]\n",
    "bestMinSamples_entropia= unique_samples[index_entropia[1]]\n",
    "\n",
    "bestEps_silhueta= unique_eps[index_silhueta[0]]\n",
    "bestEps_silhueta= unique_samples[index_silhueta[1]]\n",
    "\n",
    "bestEps_homogeneidade= unique_eps[index_homogeneidade[0]]\n",
    "bestMinSamples_homogeneidade= unique_samples[index_homogeneidade[1]]\n",
    "\n",
    "bestEps_completude= unique_eps[index_completude[0]]\n",
    "bestMinSamples_completude= unique_samples[index_completude[1]]\n",
    "\n",
    "bestEps_rand= unique_eps[index_rand[0]]\n",
    "bestMinSamples_rand= unique_samples[index_rand[1]]\n",
    "\n",
    "# Show the best Results\n",
    "print(\"DBSCAN\")\n",
    "print(f\"\\nCoesion\\n Best Score = {maxScore_coesao_dbs}\\n Radius = {bestEps_coesao}\\n Min. Points = {bestMinSamples_coesao} \")\n",
    "print(f\"\\nSeparation:\\n Best Score = {minScore_separabilidade_dbs}\\n Radius = {bestEps_separabilidade}\\n Min. Points = {bestMinSamples_separabilidade} \")\n",
    "print(f\"\\nEntropy:\\n Best Score = {minScore_entropia_dbs}\\n Radius = {bestEps_entropia}\\n Min. Points = {bestMinSamples_entropia} \")\n",
    "print(f\"\\nSilhouette:\\n Best Score = {maxScore_silhueta_dbs}\\n Radius = {bestEps_silhueta}\\n Min. Points = {bestEps_silhueta} \")\n",
    "print(f\"\\nHomogeneity:\\n Best Score = {maxScore_homogeneidade_dbs}\\n Radius = {bestEps_homogeneidade}\\n Min. Points = {bestMinSamples_homogeneidade} \")\n",
    "print(f\"\\nCompleteness:\\n Best Score = {maxScore_completude_dbs}\\n Radius = {bestEps_completude}\\n Min. Points = {bestMinSamples_completude} \")\n",
    "print(f\"\\nRand:\\n Best Score = {maxScore_rand_dbs}\\n Radius = {bestEps_rand}\\n Min. Points = {bestMinSamples_rand} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AGNES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Dictionary of linkages\n",
    "linkage_dict = {\n",
    "    0: \"ward\",\n",
    "    1: \"complete\",\n",
    "    2: \"single\"\n",
    "}\n",
    "\n",
    "# Define the parameters\n",
    "agc_parameters = {\n",
    "    'n_clusters': [i for i in range(2, 9)],\n",
    "    'linkage': [0, 1, 2]\n",
    "}\n",
    "\n",
    "# Define the size of the parameters\n",
    "cluster_size= np.shape(agc_parameters['n_clusters'])[0]\n",
    "linkage_size= np.shape(agc_parameters['linkage'])[0]\n",
    "\n",
    "# Generate the matrix of values\n",
    "agnes_value_coesao = np.zeros((cluster_size, linkage_size))\n",
    "agnes_value_separabilidade = np.zeros((cluster_size, linkage_size))\n",
    "agnes_value_entropia = np.zeros((cluster_size, linkage_size))\n",
    "agnes_value_silhueta = np.zeros((cluster_size, linkage_size))\n",
    "\n",
    "i= 0\n",
    "j= 0\n",
    "\n",
    "# Calculate the values\n",
    "for n_cluster in agc_parameters['n_clusters']:\n",
    "    j= 0\n",
    "    for linkage in agc_parameters['linkage']:\n",
    "        agc= AgglomerativeClustering(linkage=linkage_dict[linkage], n_clusters=n_cluster).fit(X)\n",
    "        agc_labels= agc.labels_\n",
    "\n",
    "        coesao = coesion_value(X, agc_labels)\n",
    "        separabilidade = separation_value(X, agc_labels)\n",
    "        entropia = entropy_value(X, agc_labels, label)\n",
    "        silhueta  = silhouette_score(X, agc_labels, metric='euclidean')\n",
    "        homogeneidade = sk.metrics.homogeneity_score(label, agc_labels)\n",
    "        completude = sk.metrics.completeness_score(label, agc_labels)\n",
    "        rand_score = sk.metrics.rand_score(label, agc_labels)\n",
    "\n",
    "        agnes_value_coesao[i, j]= coesao\n",
    "        agnes_value_separabilidade[i, j]= separabilidade\n",
    "        agnes_value_entropia[i, j]= entropia\n",
    "        agnes_value_silhueta[i, j]= silhueta\n",
    "        agnes_value_homogeneidade[i, j]= homogeneidade\n",
    "        agnes_value_completude[i, j]= completude\n",
    "        agnes_value_rand[i, j]= rand_score\n",
    "        \n",
    "        j += 1\n",
    "    i += 1\n",
    "\n",
    "template_shape= np.shape(agnes_value_coesao)\n",
    "\n",
    "# Save the best values\n",
    "maxScore_coesao_ag= np.min(agnes_value_coesao)\n",
    "minScore_separabilidade_ag= np.max(agnes_value_separabilidade)\n",
    "minScore_entropia_ag= np.min(agnes_value_entropia)\n",
    "maxScore_silhueta_ag= np.max(agnes_value_silhueta)\n",
    "maxScore_homogeneidade_ag= np.max(agnes_value_homogeneidade)\n",
    "maxScore_completude_ag= np.max(agnes_value_completude)\n",
    "maxScore_rand_ag= np.max(agnes_value_rand)\n",
    "\n",
    "index_coesao = np.unravel_index(np.argmin(agnes_value_coesao), template_shape)\n",
    "index_separabilidade = np.unravel_index(np.argmax(agnes_value_separabilidade), template_shape)\n",
    "index_entropia = np.unravel_index(np.argmin(agnes_value_entropia), template_shape)\n",
    "index_silhueta = np.unravel_index(np.argmax(agnes_value_silhueta), template_shape)\n",
    "index_homogeneidade = np.unravel_index(np.argmax(agnes_value_homogeneidade), template_shape)\n",
    "index_completude = np.unravel_index(np.argmax(agnes_value_completude), template_shape)\n",
    "index_rand = np.unravel_index(np.argmax(agnes_value_rand), template_shape)\n",
    "\n",
    "bestNAg_coesao = agc_parameters['n_clusters'][index_coesao[0]]\n",
    "bestLinkage_coesao = agc_parameters['linkage'][index_coesao[1]]\n",
    "\n",
    "bestNAg_separabilidade = agc_parameters['n_clusters'][index_separabilidade[0]]\n",
    "bestLinkage_separabilidade = agc_parameters['linkage'][index_separabilidade[1]]\n",
    "\n",
    "bestNAg_entropia = agc_parameters['n_clusters'][index_entropia[0]]\n",
    "bestLinkage_entropia=  agc_parameters['linkage'][index_entropia[1]]\n",
    "\n",
    "bestNAg_silhueta = agc_parameters['n_clusters'][index_silhueta[0]]\n",
    "bestLinkage_silhueta = agc_parameters['linkage'][index_silhueta[1]]\n",
    "\n",
    "bestNAg_homogeneidade = agc_parameters['n_clusters'][index_homogeneidade[0]]\n",
    "bestLinkage_homogeneidade = agc_parameters['linkage'][index_homogeneidade[1]]\n",
    "\n",
    "bestNAg_completude = agc_parameters['n_clusters'][index_completude[0]]\n",
    "bestLinkage_completude = agc_parameters['linkage'][index_completude[1]]\n",
    "\n",
    "bestNAg_rand = agc_parameters['n_clusters'][index_rand[0]]\n",
    "bestLinkage_rand = agc_parameters['linkage'][index_rand[1]]\n",
    "\n",
    "# Show the best Results\n",
    "print(\"AGNES\")\n",
    "print(f\"\\nCoesion\\n Best Score = {maxScore_coesao_ag}\\n {bestNAg_coesao} Clusters {linkage_dict[bestLinkage_coesao]}\\n\")\n",
    "print(f\"\\nSeparation:\\n Best Score = {minScore_separabilidade_ag}\\n {bestNAg_separabilidade} Clusters {linkage_dict[bestLinkage_separabilidade]}\\n\")\n",
    "print(f\"\\nEntropy:\\n Best Score = {minScore_entropia_ag}\\n {bestNAg_entropia} Clusters {linkage_dict[bestLinkage_entropia]}\\n\")\n",
    "print(f\"\\nSilhouette:\\n Best Score = {maxScore_silhueta_ag}\\n {bestNAg_silhueta} Clusters\\n {linkage_dict[bestLinkage_silhueta]} Similarity\\n\")\n",
    "print(f\"\\nHomogeneity:\\n Best Score = {maxScore_homogeneidade_ag}\\n {bestNAg_homogeneidade} Clusters\\n {linkage_dict[bestLinkage_homogeneidade]} Similarity\\n\")\n",
    "print(f\"\\nCompleteness:\\n Best Score = {maxScore_completude_ag}\\n {bestNAg_completude} Clusters\\n {linkage_dict[bestLinkage_completude]} Similarity\\n\")\n",
    "print(f\"\\nRand:\\n Best Score = {maxScore_rand_ag}\\n {bestNAg_rand} Clusters\\n {linkage_dict[bestLinkage_rand]} Similarity\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the Clusters Systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "coesao_row= [maxScore_coesao_kmeans,maxScore_coesao_dbs,maxScore_coesao_ag]\n",
    "\n",
    "separabilidade_row= [minScore_separabilidade_kmeans,minScore_separabilidade_dbs,minScore_separabilidade_ag]\n",
    "\n",
    "entropia_row= [minScore_entropia_kmeans,minScore_entropia_dbs,minScore_entropia_ag]\n",
    "\n",
    "silhouette_row= [maxScore_silhueta_kmeans,maxScore_silhueta_dbs,maxScore_silhueta_ag]\n",
    "\n",
    "homegeneidade_row = [maxScore_homogeneidade_kmeans,maxScore_homogeneidade_dbs,maxScore_homogeneidade_ag]\n",
    "\n",
    "completude_row = [maxScore_completude_kmeans,maxScore_completude_dbs,maxScore_completude_ag]\n",
    "\n",
    "rand_row = [maxScore_rand_kmeans,maxScore_rand_dbs,maxScore_rand_ag]\n",
    "\n",
    "storage= [\n",
    "    coesao_row,\n",
    "    separabilidade_row,\n",
    "    entropia_row,\n",
    "    silhouette_row,\n",
    "    homegeneidade_row,\n",
    "    completude_row,\n",
    "    rand_row\n",
    "]\n",
    "\n",
    "pd.DataFrame(storage, index=[\"Coesão\", \"Separação\", \"Entropia\", \"Silhueta\", \"Homogeineidade\", \"Completude\", \"Rand\"], columns=[\"KMeans\", \"DBScan\", \"Agnes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '/bin/python3' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
